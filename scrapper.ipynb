{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "\n",
    "def scrape_all_pages(base_url, start_page, num_pages):\n",
    "    all_results = pd.Series()\n",
    "\n",
    "    for page_number in range(start_page, start_page + num_pages):\n",
    "        url = f\"{base_url}/page/{page_number}/\"\n",
    "\n",
    "        # Navigate to the page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Find the elements that contain the links to professor's pages\n",
    "        elements = driver.find_elements(By.XPATH, \"/html/body/section[4]/div/div/div/div/div/a\")\n",
    "\n",
    "        # Extract URLs from the elements\n",
    "        professor_urls = [element.get_attribute('href') for element in elements]\n",
    "        professor_urls = pd.Series(professor_urls)\n",
    "        all_results = pd.concat([all_results, professor_urls], axis = 0)\n",
    "    return all_results\n",
    "\n",
    "# Usage\n",
    "base_url = #url of the institution faculty institution website\n",
    "num_pages = 17  # Set the number of pages you want to scrape\n",
    "\n",
    "urls = scrape_all_pages(base_url, 2, num_pages)\n",
    "\n",
    "def get_bio(url):\n",
    "    driver.get(url)\n",
    "\n",
    "    # Extracting the name\n",
    "    name = driver.find_element(By.XPATH, '/html/body/section[2]/div[1]/div/div/div/div/h2').text\n",
    "\n",
    "    # Extracting the bio\n",
    "    bio_containers = driver.find_elements(By.XPATH, '/html/body/section[3]/div/div/div/div[2]/div[1]/div/div/div')\n",
    "    bio_texts = []\n",
    "\n",
    "    for container in bio_containers:\n",
    "        paragraphs = container.find_elements(By.TAG_NAME, 'p')\n",
    "        for paragraph in paragraphs:\n",
    "            bio_texts.append(paragraph.text)\n",
    "\n",
    "    # Concatenate paragraphs to form a single bio text\n",
    "    bio = ' '.join(bio_texts)\n",
    "\n",
    "    return {'name': name, 'bio': bio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bios = {} #Initiallising a dictionary for all the data\n",
    "\n",
    "c=0\n",
    "for _, url in urls.iteritems(): #looking through the url list\n",
    "    all_bios[c] = get_bio(url)\n",
    "    c+=1\n",
    "\n",
    "all_bios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
